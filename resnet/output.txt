nohup: ignoring input
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
{'boxes': tensor([[ 99.7500, 112.4922, 102.8125, 125.9453],
        [148.7500, 117.2500, 151.1562, 126.0000]]), 'labels': tensor([1, 1]), 'orig_size': tensor([ 128, 1024], dtype=torch.int32)}
♫Training Montage♫ by Vince DiCola starts playing...
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:41<2:16:05, 41.03s/it]  1%|          | 2/200 [01:21<2:14:12, 40.67s/it]  2%|▏         | 3/200 [02:01<2:13:09, 40.56s/it]  2%|▏         | 4/200 [02:42<2:12:36, 40.59s/it]  2%|▎         | 5/200 [03:23<2:12:00, 40.62s/it]  3%|▎         | 6/200 [04:07<2:14:59, 41.75s/it]  4%|▎         | 7/200 [04:50<2:16:28, 42.43s/it]  4%|▍         | 8/200 [05:34<2:17:12, 42.88s/it]  4%|▍         | 9/200 [06:18<2:17:26, 43.17s/it]  5%|▌         | 10/200 [07:02<2:17:34, 43.44s/it]  6%|▌         | 11/200 [07:50<2:20:40, 44.66s/it]  6%|▌         | 12/200 [08:37<2:22:30, 45.48s/it]  6%|▋         | 13/200 [09:24<2:23:27, 46.03s/it]  7%|▋         | 14/200 [10:12<2:23:54, 46.42s/it]  8%|▊         | 15/200 [10:59<2:23:57, 46.69s/it]  8%|▊         | 16/200 [11:51<2:28:22, 48.38s/it]  8%|▊         | 17/200 [12:44<2:31:11, 49.57s/it]  9%|▉         | 18/200 [13:36<2:32:50, 50.39s/it] 10%|▉         | 19/200 [14:28<2:33:41, 50.95s/it] 10%|█         | 20/200 [15:20<2:34:01, 51.34s/it] 10%|█         | 21/200 [16:13<2:34:03, 51.64s/it] 11%|█         | 22/200 [17:05<2:33:43, 51.82s/it] 12%|█▏        | 23/200 [17:57<2:33:15, 51.95s/it] 12%|█▏        | 24/200 [18:49<2:32:39, 52.04s/it] 12%|█▎        | 25/200 [19:42<2:32:04, 52.14s/it] 13%|█▎        | 26/200 [20:34<2:31:19, 52.18s/it] 14%|█▎        | 27/200 [21:26<2:30:30, 52.20s/it] 14%|█▍        | 28/200 [22:19<2:29:41, 52.22s/it] 14%|█▍        | 29/200 [23:11<2:28:52, 52.24s/it] 15%|█▌        | 30/200 [24:03<2:28:06, 52.27s/it] 16%|█▌        | 31/200 [24:55<2:27:15, 52.28s/it] 16%|█▌        | 32/200 [25:48<2:26:22, 52.28s/it] 16%|█▋        | 33/200 [26:40<2:25:30, 52.28s/it] 17%|█▋        | 34/200 [27:32<2:24:41, 52.30s/it] 18%|█▊        | 35/200 [28:25<2:23:49, 52.30s/it] 18%|█▊        | 36/200 [29:17<2:22:54, 52.29s/it] 18%|█▊        | 37/200 [30:09<2:22:02, 52.28s/it] 19%|█▉        | 38/200 [31:02<2:21:13, 52.31s/it] 20%|█▉        | 39/200 [31:54<2:20:20, 52.30s/it] 20%|██        | 40/200 [32:46<2:19:27, 52.29s/it] 20%|██        | 41/200 [33:38<2:18:34, 52.29s/it] 21%|██        | 42/200 [34:31<2:17:45, 52.31s/it] 22%|██▏       | 43/200 [35:23<2:16:51, 52.30s/it] 22%|██▏       | 44/200 [36:15<2:15:56, 52.29s/it] 22%|██▎       | 45/200 [37:08<2:15:02, 52.27s/it] 23%|██▎       | 46/200 [38:00<2:14:09, 52.27s/it] 24%|██▎       | 47/200 [38:52<2:13:17, 52.27s/it] 24%|██▍       | 48/200 [39:44<2:12:24, 52.26s/it] 24%|██▍       | 49/200 [40:37<2:11:30, 52.26s/it] 25%|██▌       | 50/200 [41:29<2:10:36, 52.24s/it] 26%|██▌       | 51/200 [42:21<2:09:48, 52.27s/it] 26%|██▌       | 52/200 [43:13<2:08:55, 52.27s/it] 26%|██▋       | 53/200 [44:06<2:08:01, 52.26s/it] 27%|██▋       | 54/200 [44:58<2:07:08, 52.25s/it] 28%|██▊       | 55/200 [45:50<2:06:14, 52.24s/it] 28%|██▊       | 56/200 [46:42<2:05:25, 52.26s/it] 28%|██▊       | 57/200 [47:35<2:04:31, 52.25s/it] 29%|██▉       | 58/200 [48:27<2:03:37, 52.24s/it] 30%|██▉       | 59/200 [49:19<2:02:44, 52.23s/it] 30%|███       | 60/200 [50:11<2:01:51, 52.23s/it] 30%|███       | 61/200 [51:04<2:01:02, 52.25s/it] 31%|███       | 62/200 [51:56<2:00:09, 52.24s/it] 32%|███▏      | 63/200 [52:48<1:59:16, 52.23s/it] 32%|███▏      | 64/200 [53:40<1:58:23, 52.23s/it] 32%|███▎      | 65/200 [54:33<1:57:34, 52.25s/it] 33%|███▎      | 66/200 [55:25<1:56:40, 52.24s/it] 34%|███▎      | 67/200 [56:17<1:55:46, 52.23s/it] 34%|███▍      | 68/200 [57:09<1:54:57, 52.26s/it] 34%|███▍      | 69/200 [58:02<1:54:05, 52.26s/it] 35%|███▌      | 70/200 [58:54<1:53:12, 52.25s/it] 36%|███▌      | 71/200 [59:46<1:52:18, 52.23s/it] 36%|███▌      | 72/200 [1:00:38<1:51:29, 52.26s/it] 36%|███▋      | 73/200 [1:01:31<1:50:36, 52.26s/it] 37%|███▋      | 74/200 [1:02:23<1:49:43, 52.25s/it] 38%|███▊      | 75/200 [1:03:15<1:48:50, 52.25s/it] 38%|███▊      | 76/200 [1:04:07<1:48:00, 52.26s/it] 38%|███▊      | 77/200 [1:05:00<1:47:08, 52.26s/it] 39%|███▉      | 78/200 [1:05:52<1:46:13, 52.25s/it] 40%|███▉      | 79/200 [1:06:44<1:45:16, 52.20s/it] 40%|████      | 80/200 [1:07:36<1:44:21, 52.18s/it] 40%|████      | 81/200 [1:08:28<1:43:27, 52.16s/it] 41%|████      | 82/200 [1:09:20<1:42:31, 52.13s/it] 42%|████▏     | 83/200 [1:10:12<1:41:37, 52.12s/it] 42%|████▏     | 84/200 [1:11:04<1:40:46, 52.12s/it] 42%|████▎     | 85/200 [1:11:56<1:39:53, 52.11s/it] 43%|████▎     | 86/200 [1:12:49<1:39:00, 52.11s/it] 44%|████▎     | 87/200 [1:13:41<1:38:07, 52.10s/it] 44%|████▍     | 88/200 [1:14:33<1:37:14, 52.09s/it] 44%|████▍     | 89/200 [1:15:25<1:36:25, 52.12s/it] 45%|████▌     | 90/200 [1:16:17<1:35:33, 52.12s/it] 46%|████▌     | 91/200 [1:17:09<1:34:40, 52.12s/it] 46%|████▌     | 92/200 [1:18:01<1:33:48, 52.12s/it] 46%|████▋     | 93/200 [1:18:53<1:32:58, 52.13s/it] 47%|████▋     | 94/200 [1:19:46<1:32:04, 52.12s/it] 48%|████▊     | 95/200 [1:20:38<1:31:12, 52.12s/it] 48%|████▊     | 96/200 [1:21:30<1:30:19, 52.11s/it] 48%|████▊     | 97/200 [1:22:22<1:29:29, 52.13s/it] 49%|████▉     | 98/200 [1:23:14<1:28:36, 52.12s/it] 50%|████▉     | 99/200 [1:24:06<1:27:43, 52.11s/it] 50%|█████     | 100/200 [1:24:58<1:26:51, 52.11s/it] 50%|█████     | 101/200 [1:25:50<1:25:59, 52.11s/it] 51%|█████     | 102/200 [1:26:43<1:25:09, 52.14s/it] 52%|█████▏    | 103/200 [1:27:35<1:24:16, 52.13s/it] 52%|█████▏    | 104/200 [1:28:27<1:23:24, 52.13s/it] 52%|█████▎    | 105/200 [1:29:19<1:22:31, 52.12s/it] 53%|█████▎    | 106/200 [1:30:11<1:21:41, 52.14s/it] 54%|█████▎    | 107/200 [1:31:03<1:20:48, 52.13s/it] 54%|█████▍    | 108/200 [1:31:55<1:19:55, 52.12s/it] 55%|█████▍    | 109/200 [1:32:47<1:19:05, 52.15s/it] 55%|█████▌    | 110/200 [1:33:40<1:18:11, 52.13s/it] 56%|█████▌    | 111/200 [1:34:32<1:17:18, 52.11s/it] 56%|█████▌    | 112/200 [1:35:24<1:16:25, 52.11s/it] 56%|█████▋    | 113/200 [1:36:16<1:15:37, 52.15s/it] 57%|█████▋    | 114/200 [1:37:08<1:14:44, 52.15s/it] 57%|█████▊    | 115/200 [1:38:00<1:13:51, 52.13s/it] 58%|█████▊    | 116/200 [1:38:52<1:12:58, 52.12s/it] 58%|█████▊    | 117/200 [1:39:44<1:12:05, 52.11s/it] 59%|█████▉    | 118/200 [1:40:37<1:11:15, 52.14s/it] 60%|█████▉    | 119/200 [1:41:29<1:10:23, 52.14s/it] 60%|██████    | 120/200 [1:42:21<1:09:30, 52.13s/it] 60%|██████    | 121/200 [1:43:13<1:08:38, 52.13s/it] 61%|██████    | 122/200 [1:44:05<1:07:46, 52.14s/it] 62%|██████▏   | 123/200 [1:44:57<1:06:53, 52.13s/it] 62%|██████▏   | 124/200 [1:45:49<1:06:01, 52.12s/it] 62%|██████▎   | 125/200 [1:46:41<1:05:08, 52.11s/it] 63%|██████▎   | 126/200 [1:47:34<1:04:17, 52.13s/it] 64%|██████▎   | 127/200 [1:48:26<1:03:25, 52.13s/it] 64%|██████▍   | 128/200 [1:49:18<1:02:33, 52.13s/it] 64%|██████▍   | 129/200 [1:50:10<1:01:40, 52.12s/it] 65%|██████▌   | 130/200 [1:51:02<1:00:49, 52.14s/it] 66%|██████▌   | 131/200 [1:51:54<59:56, 52.12s/it]   66%|██████▌   | 132/200 [1:52:46<59:03, 52.11s/it] 66%|██████▋   | 133/200 [1:53:38<58:11, 52.11s/it] 67%|██████▋   | 134/200 [1:54:31<57:21, 52.14s/it] 68%|██████▊   | 135/200 [1:55:23<56:28, 52.14s/it] 68%|██████▊   | 136/200 [1:56:15<55:41, 52.20s/it] 68%|██████▊   | 137/200 [1:57:07<54:51, 52.25s/it] 69%|██████▉   | 138/200 [1:58:00<54:02, 52.30s/it] 70%|██████▉   | 139/200 [1:58:52<53:10, 52.30s/it] 70%|███████   | 140/200 [1:59:45<52:18, 52.32s/it] 70%|███████   | 141/200 [2:00:37<51:27, 52.32s/it] 71%|███████   | 142/200 [2:01:29<50:34, 52.32s/it] 72%|███████▏  | 143/200 [2:02:22<49:44, 52.36s/it] 72%|███████▏  | 144/200 [2:03:14<48:51, 52.36s/it] 72%|███████▎  | 145/200 [2:04:06<47:59, 52.35s/it] 73%|███████▎  | 146/200 [2:04:59<47:08, 52.38s/it] 74%|███████▎  | 147/200 [2:05:51<46:15, 52.37s/it] 74%|███████▍  | 148/200 [2:06:43<45:21, 52.34s/it] 74%|███████▍  | 149/200 [2:07:36<44:28, 52.33s/it] 75%|███████▌  | 150/200 [2:08:28<43:36, 52.34s/it] 76%|███████▌  | 151/200 [2:09:20<42:44, 52.33s/it] 76%|███████▌  | 152/200 [2:10:13<41:51, 52.32s/it] 76%|███████▋  | 153/200 [2:11:05<40:58, 52.32s/it] 77%|███████▋  | 154/200 [2:11:57<40:07, 52.34s/it] 78%|███████▊  | 155/200 [2:12:50<39:14, 52.32s/it] 78%|███████▊  | 156/200 [2:13:42<38:21, 52.31s/it] 78%|███████▊  | 157/200 [2:14:34<37:28, 52.30s/it] 79%|███████▉  | 158/200 [2:15:27<36:37, 52.32s/it] 80%|███████▉  | 159/200 [2:16:19<35:45, 52.32s/it] 80%|████████  | 160/200 [2:17:11<34:52, 52.31s/it] 80%|████████  | 161/200 [2:18:03<33:59, 52.30s/it] 81%|████████  | 162/200 [2:18:56<33:08, 52.32s/it] 82%|████████▏ | 163/200 [2:19:48<32:15, 52.30s/it] 82%|████████▏ | 164/200 [2:20:40<31:22, 52.30s/it] 82%|████████▎ | 165/200 [2:21:33<30:30, 52.29s/it] 83%|████████▎ | 166/200 [2:22:25<29:38, 52.32s/it] 84%|████████▎ | 167/200 [2:23:17<28:45, 52.30s/it] 84%|████████▍ | 168/200 [2:24:10<27:53, 52.29s/it] 84%|████████▍ | 169/200 [2:25:02<27:00, 52.29s/it] 85%|████████▌ | 170/200 [2:25:54<26:09, 52.31s/it] 86%|████████▌ | 171/200 [2:26:47<25:16, 52.31s/it] 86%|████████▌ | 172/200 [2:27:39<24:24, 52.30s/it] 86%|████████▋ | 173/200 [2:28:31<23:31, 52.29s/it] 87%|████████▋ | 174/200 [2:29:23<22:39, 52.31s/it] 88%|████████▊ | 175/200 [2:30:16<21:47, 52.29s/it] 88%|████████▊ | 176/200 [2:31:08<20:54, 52.28s/it] 88%|████████▊ | 177/200 [2:32:00<20:02, 52.28s/it] 89%|████████▉ | 178/200 [2:32:53<19:10, 52.30s/it] 90%|████████▉ | 179/200 [2:33:45<18:18, 52.29s/it] 90%|█████████ | 180/200 [2:34:37<17:25, 52.28s/it] 90%|█████████ | 181/200 [2:35:29<16:33, 52.27s/it] 91%|█████████ | 182/200 [2:36:22<15:40, 52.27s/it] 92%|█████████▏| 183/200 [2:37:14<14:49, 52.30s/it] 92%|█████████▏| 184/200 [2:38:06<13:56, 52.30s/it] 92%|█████████▎| 185/200 [2:38:59<13:04, 52.29s/it] 93%|█████████▎| 186/200 [2:39:51<12:11, 52.28s/it] 94%|█████████▎| 187/200 [2:40:43<11:20, 52.31s/it] 94%|█████████▍| 188/200 [2:41:35<10:27, 52.29s/it] 94%|█████████▍| 189/200 [2:42:28<09:35, 52.28s/it] 95%|█████████▌| 190/200 [2:43:20<08:42, 52.27s/it] 96%|█████████▌| 191/200 [2:44:12<07:50, 52.30s/it] 96%|█████████▌| 192/200 [2:45:05<06:58, 52.30s/it] 96%|█████████▋| 193/200 [2:45:57<06:05, 52.28s/it] 97%|█████████▋| 194/200 [2:46:49<05:13, 52.28s/it] 98%|█████████▊| 195/200 [2:47:41<04:21, 52.31s/it] 98%|█████████▊| 196/200 [2:48:34<03:29, 52.30s/it] 98%|█████████▊| 197/200 [2:49:26<02:36, 52.29s/it] 99%|█████████▉| 198/200 [2:50:18<01:44, 52.30s/it]100%|█████████▉| 199/200 [2:51:11<00:52, 52.30s/it]100%|██████████| 200/200 [2:52:03<00:00, 52.29s/it]100%|██████████| 200/200 [2:52:03<00:00, 51.62s/it]
Epoch 0 - Train Loss: 0.468386, Val Loss: 0.303812, mAP@.5:.95: 0.0023, mAP@.5: 0.0015
Epoch 1 - Train Loss: 0.299354, Val Loss: 0.281852, mAP@.5:.95: 0.0000, mAP@.5: 0.0000
Epoch 2 - Train Loss: 0.286785, Val Loss: 0.286380, mAP@.5:.95: 0.0000, mAP@.5: 0.0000
Epoch 3 - Train Loss: 0.281526, Val Loss: 0.295353, mAP@.5:.95: 0.0084, mAP@.5: 0.0078
Epoch 4 - Train Loss: 0.277387, Val Loss: 0.283917, mAP@.5:.95: 0.0117, mAP@.5: 0.0107
Unfreezing layer 3. Cool party!
Epoch 5 - Train Loss: 0.268787, Val Loss: 0.267902, mAP@.5:.95: 0.0131, mAP@.5: 0.0104
Epoch 6 - Train Loss: 0.264220, Val Loss: 0.265599, mAP@.5:.95: 0.0161, mAP@.5: 0.0129
Epoch 7 - Train Loss: 0.265437, Val Loss: 0.258952, mAP@.5:.95: 0.0209, mAP@.5: 0.0200
Epoch 8 - Train Loss: 0.262965, Val Loss: 0.265657, mAP@.5:.95: 0.0270, mAP@.5: 0.0289
Epoch 9 - Train Loss: 0.261323, Val Loss: 0.254869, mAP@.5:.95: 0.0342, mAP@.5: 0.0358
Unfreezing layer 2. What killed the dinosaurs? The Ice Age!
Epoch 10 - Train Loss: 0.256032, Val Loss: 0.249029, mAP@.5:.95: 0.0305, mAP@.5: 0.0296
Epoch 11 - Train Loss: 0.255329, Val Loss: 0.247857, mAP@.5:.95: 0.0383, mAP@.5: 0.0351
Epoch 12 - Train Loss: 0.251306, Val Loss: 0.247138, mAP@.5:.95: 0.0329, mAP@.5: 0.0328
Epoch 13 - Train Loss: 0.253041, Val Loss: 0.251418, mAP@.5:.95: 0.0303, mAP@.5: 0.0296
Epoch 14 - Train Loss: 0.251706, Val Loss: 0.247979, mAP@.5:.95: 0.0322, mAP@.5: 0.0345
Unfreezing the entire backbone. Everybody chill!
Epoch 15 - Train Loss: 0.249625, Val Loss: 0.247670, mAP@.5:.95: 0.0315, mAP@.5: 0.0274
Epoch 16 - Train Loss: 0.250848, Val Loss: 0.247322, mAP@.5:.95: 0.0376, mAP@.5: 0.0321
Epoch 17 - Train Loss: 0.250107, Val Loss: 0.245580, mAP@.5:.95: 0.0349, mAP@.5: 0.0320
Epoch 18 - Train Loss: 0.250595, Val Loss: 0.245449, mAP@.5:.95: 0.0349, mAP@.5: 0.0309
Epoch 19 - Train Loss: 0.249391, Val Loss: 0.247789, mAP@.5:.95: 0.0363, mAP@.5: 0.0345
Epoch 20 - Train Loss: 0.250866, Val Loss: 0.246701, mAP@.5:.95: 0.0353, mAP@.5: 0.0330
Epoch 21 - Train Loss: 0.248445, Val Loss: 0.245071, mAP@.5:.95: 0.0358, mAP@.5: 0.0320
Epoch 22 - Train Loss: 0.248239, Val Loss: 0.246476, mAP@.5:.95: 0.0330, mAP@.5: 0.0240
Epoch 23 - Train Loss: 0.250609, Val Loss: 0.245476, mAP@.5:.95: 0.0377, mAP@.5: 0.0364
Epoch 24 - Train Loss: 0.250377, Val Loss: 0.244072, mAP@.5:.95: 0.0366, mAP@.5: 0.0320
Epoch 25 - Train Loss: 0.250878, Val Loss: 0.247454, mAP@.5:.95: 0.0345, mAP@.5: 0.0289
Epoch 26 - Train Loss: 0.250065, Val Loss: 0.248075, mAP@.5:.95: 0.0351, mAP@.5: 0.0339
Epoch 27 - Train Loss: 0.248629, Val Loss: 0.247244, mAP@.5:.95: 0.0382, mAP@.5: 0.0348
Epoch 28 - Train Loss: 0.251379, Val Loss: 0.245595, mAP@.5:.95: 0.0388, mAP@.5: 0.0357
Epoch 29 - Train Loss: 0.249823, Val Loss: 0.244447, mAP@.5:.95: 0.0381, mAP@.5: 0.0335
Epoch 30 - Train Loss: 0.250517, Val Loss: 0.245509, mAP@.5:.95: 0.0387, mAP@.5: 0.0356
Epoch 31 - Train Loss: 0.248228, Val Loss: 0.243861, mAP@.5:.95: 0.0379, mAP@.5: 0.0355
Epoch 32 - Train Loss: 0.250071, Val Loss: 0.246428, mAP@.5:.95: 0.0362, mAP@.5: 0.0321
Epoch 33 - Train Loss: 0.249977, Val Loss: 0.244234, mAP@.5:.95: 0.0380, mAP@.5: 0.0325
Epoch 34 - Train Loss: 0.250016, Val Loss: 0.243709, mAP@.5:.95: 0.0399, mAP@.5: 0.0361
Epoch 35 - Train Loss: 0.248728, Val Loss: 0.245465, mAP@.5:.95: 0.0369, mAP@.5: 0.0312
Epoch 36 - Train Loss: 0.249187, Val Loss: 0.245712, mAP@.5:.95: 0.0384, mAP@.5: 0.0394
Epoch 37 - Train Loss: 0.250733, Val Loss: 0.244470, mAP@.5:.95: 0.0388, mAP@.5: 0.0374
Epoch 38 - Train Loss: 0.248510, Val Loss: 0.244960, mAP@.5:.95: 0.0389, mAP@.5: 0.0356
Epoch 39 - Train Loss: 0.249729, Val Loss: 0.245632, mAP@.5:.95: 0.0386, mAP@.5: 0.0432
Epoch 40 - Train Loss: 0.249888, Val Loss: 0.244224, mAP@.5:.95: 0.0380, mAP@.5: 0.0339
Epoch 41 - Train Loss: 0.250098, Val Loss: 0.244546, mAP@.5:.95: 0.0338, mAP@.5: 0.0271
Epoch 42 - Train Loss: 0.248528, Val Loss: 0.244139, mAP@.5:.95: 0.0385, mAP@.5: 0.0323
Epoch 43 - Train Loss: 0.249861, Val Loss: 0.245526, mAP@.5:.95: 0.0375, mAP@.5: 0.0348
Epoch 44 - Train Loss: 0.249914, Val Loss: 0.245753, mAP@.5:.95: 0.0385, mAP@.5: 0.0358
Epoch 45 - Train Loss: 0.247955, Val Loss: 0.246192, mAP@.5:.95: 0.0409, mAP@.5: 0.0424
Epoch 46 - Train Loss: 0.248459, Val Loss: 0.244944, mAP@.5:.95: 0.0350, mAP@.5: 0.0272
Epoch 47 - Train Loss: 0.249720, Val Loss: 0.245977, mAP@.5:.95: 0.0404, mAP@.5: 0.0417
Epoch 48 - Train Loss: 0.247519, Val Loss: 0.246382, mAP@.5:.95: 0.0387, mAP@.5: 0.0363
Epoch 49 - Train Loss: 0.247916, Val Loss: 0.245544, mAP@.5:.95: 0.0402, mAP@.5: 0.0388
Epoch 50 - Train Loss: 0.249823, Val Loss: 0.245902, mAP@.5:.95: 0.0403, mAP@.5: 0.0377
Epoch 51 - Train Loss: 0.247384, Val Loss: 0.242613, mAP@.5:.95: 0.0391, mAP@.5: 0.0363
Epoch 52 - Train Loss: 0.248236, Val Loss: 0.248351, mAP@.5:.95: 0.0387, mAP@.5: 0.0347
Epoch 53 - Train Loss: 0.248970, Val Loss: 0.245422, mAP@.5:.95: 0.0401, mAP@.5: 0.0369
Epoch 54 - Train Loss: 0.246670, Val Loss: 0.246033, mAP@.5:.95: 0.0403, mAP@.5: 0.0362
Epoch 55 - Train Loss: 0.248582, Val Loss: 0.245394, mAP@.5:.95: 0.0416, mAP@.5: 0.0415
Epoch 56 - Train Loss: 0.248552, Val Loss: 0.244335, mAP@.5:.95: 0.0416, mAP@.5: 0.0404
Epoch 57 - Train Loss: 0.248027, Val Loss: 0.244946, mAP@.5:.95: 0.0413, mAP@.5: 0.0400
Epoch 58 - Train Loss: 0.248842, Val Loss: 0.243676, mAP@.5:.95: 0.0403, mAP@.5: 0.0384
Epoch 59 - Train Loss: 0.247504, Val Loss: 0.243359, mAP@.5:.95: 0.0421, mAP@.5: 0.0353
Epoch 60 - Train Loss: 0.248600, Val Loss: 0.245460, mAP@.5:.95: 0.0419, mAP@.5: 0.0354
Epoch 61 - Train Loss: 0.247928, Val Loss: 0.244111, mAP@.5:.95: 0.0408, mAP@.5: 0.0412
Epoch 62 - Train Loss: 0.249163, Val Loss: 0.244502, mAP@.5:.95: 0.0400, mAP@.5: 0.0393
Epoch 63 - Train Loss: 0.248762, Val Loss: 0.244994, mAP@.5:.95: 0.0396, mAP@.5: 0.0347
Epoch 64 - Train Loss: 0.247021, Val Loss: 0.244369, mAP@.5:.95: 0.0413, mAP@.5: 0.0393
Epoch 65 - Train Loss: 0.247105, Val Loss: 0.243687, mAP@.5:.95: 0.0403, mAP@.5: 0.0387
Epoch 66 - Train Loss: 0.247734, Val Loss: 0.246923, mAP@.5:.95: 0.0437, mAP@.5: 0.0443
Epoch 67 - Train Loss: 0.248512, Val Loss: 0.245756, mAP@.5:.95: 0.0415, mAP@.5: 0.0405
Epoch 68 - Train Loss: 0.248922, Val Loss: 0.244560, mAP@.5:.95: 0.0420, mAP@.5: 0.0396
Epoch 69 - Train Loss: 0.246955, Val Loss: 0.244793, mAP@.5:.95: 0.0428, mAP@.5: 0.0421
Epoch 70 - Train Loss: 0.249587, Val Loss: 0.246335, mAP@.5:.95: 0.0400, mAP@.5: 0.0404
Epoch 71 - Train Loss: 0.247095, Val Loss: 0.245484, mAP@.5:.95: 0.0429, mAP@.5: 0.0432
Epoch 72 - Train Loss: 0.246850, Val Loss: 0.246353, mAP@.5:.95: 0.0403, mAP@.5: 0.0342
Epoch 73 - Train Loss: 0.247475, Val Loss: 0.244536, mAP@.5:.95: 0.0420, mAP@.5: 0.0411
Epoch 74 - Train Loss: 0.247509, Val Loss: 0.244759, mAP@.5:.95: 0.0439, mAP@.5: 0.0419
Epoch 75 - Train Loss: 0.247621, Val Loss: 0.244647, mAP@.5:.95: 0.0399, mAP@.5: 0.0380
Epoch 76 - Train Loss: 0.248081, Val Loss: 0.243355, mAP@.5:.95: 0.0415, mAP@.5: 0.0401
Epoch 77 - Train Loss: 0.247518, Val Loss: 0.245903, mAP@.5:.95: 0.0415, mAP@.5: 0.0395
Epoch 78 - Train Loss: 0.248639, Val Loss: 0.244982, mAP@.5:.95: 0.0418, mAP@.5: 0.0420
Epoch 79 - Train Loss: 0.247703, Val Loss: 0.245067, mAP@.5:.95: 0.0404, mAP@.5: 0.0363
Epoch 80 - Train Loss: 0.248682, Val Loss: 0.245386, mAP@.5:.95: 0.0438, mAP@.5: 0.0423
Epoch 81 - Train Loss: 0.247765, Val Loss: 0.244603, mAP@.5:.95: 0.0406, mAP@.5: 0.0384
Epoch 82 - Train Loss: 0.247559, Val Loss: 0.243704, mAP@.5:.95: 0.0424, mAP@.5: 0.0407
Epoch 83 - Train Loss: 0.246280, Val Loss: 0.245325, mAP@.5:.95: 0.0423, mAP@.5: 0.0401
Epoch 84 - Train Loss: 0.245020, Val Loss: 0.243957, mAP@.5:.95: 0.0415, mAP@.5: 0.0398
Epoch 85 - Train Loss: 0.247778, Val Loss: 0.243033, mAP@.5:.95: 0.0424, mAP@.5: 0.0420
Epoch 86 - Train Loss: 0.246601, Val Loss: 0.244216, mAP@.5:.95: 0.0408, mAP@.5: 0.0418
Epoch 87 - Train Loss: 0.246078, Val Loss: 0.243574, mAP@.5:.95: 0.0432, mAP@.5: 0.0415
Epoch 88 - Train Loss: 0.246562, Val Loss: 0.244614, mAP@.5:.95: 0.0440, mAP@.5: 0.0441
Epoch 89 - Train Loss: 0.247380, Val Loss: 0.242704, mAP@.5:.95: 0.0444, mAP@.5: 0.0442
Epoch 90 - Train Loss: 0.248484, Val Loss: 0.242883, mAP@.5:.95: 0.0453, mAP@.5: 0.0438
Epoch 91 - Train Loss: 0.248797, Val Loss: 0.246701, mAP@.5:.95: 0.0422, mAP@.5: 0.0409
Epoch 92 - Train Loss: 0.248318, Val Loss: 0.243168, mAP@.5:.95: 0.0414, mAP@.5: 0.0361
Epoch 93 - Train Loss: 0.247507, Val Loss: 0.246696, mAP@.5:.95: 0.0429, mAP@.5: 0.0355
Epoch 94 - Train Loss: 0.247460, Val Loss: 0.243701, mAP@.5:.95: 0.0442, mAP@.5: 0.0456
Epoch 95 - Train Loss: 0.248404, Val Loss: 0.244948, mAP@.5:.95: 0.0441, mAP@.5: 0.0455
Epoch 96 - Train Loss: 0.246544, Val Loss: 0.244596, mAP@.5:.95: 0.0448, mAP@.5: 0.0409
Epoch 97 - Train Loss: 0.248957, Val Loss: 0.243931, mAP@.5:.95: 0.0410, mAP@.5: 0.0349
Epoch 98 - Train Loss: 0.247938, Val Loss: 0.243504, mAP@.5:.95: 0.0446, mAP@.5: 0.0430
Epoch 99 - Train Loss: 0.247800, Val Loss: 0.245033, mAP@.5:.95: 0.0407, mAP@.5: 0.0351
Epoch 100 - Train Loss: 0.247444, Val Loss: 0.243277, mAP@.5:.95: 0.0449, mAP@.5: 0.0446
Epoch 101 - Train Loss: 0.246046, Val Loss: 0.245129, mAP@.5:.95: 0.0423, mAP@.5: 0.0403
Epoch 102 - Train Loss: 0.247419, Val Loss: 0.243899, mAP@.5:.95: 0.0427, mAP@.5: 0.0399
Epoch 103 - Train Loss: 0.248316, Val Loss: 0.242627, mAP@.5:.95: 0.0418, mAP@.5: 0.0383
Epoch 104 - Train Loss: 0.245960, Val Loss: 0.243069, mAP@.5:.95: 0.0434, mAP@.5: 0.0413
Epoch 105 - Train Loss: 0.245525, Val Loss: 0.243158, mAP@.5:.95: 0.0413, mAP@.5: 0.0351
Epoch 106 - Train Loss: 0.247467, Val Loss: 0.242977, mAP@.5:.95: 0.0443, mAP@.5: 0.0446
Epoch 107 - Train Loss: 0.246854, Val Loss: 0.244021, mAP@.5:.95: 0.0432, mAP@.5: 0.0406
Epoch 108 - Train Loss: 0.247166, Val Loss: 0.242010, mAP@.5:.95: 0.0458, mAP@.5: 0.0445
Epoch 109 - Train Loss: 0.245805, Val Loss: 0.242908, mAP@.5:.95: 0.0418, mAP@.5: 0.0413
Epoch 110 - Train Loss: 0.247326, Val Loss: 0.242199, mAP@.5:.95: 0.0433, mAP@.5: 0.0419
Epoch 111 - Train Loss: 0.247399, Val Loss: 0.244653, mAP@.5:.95: 0.0404, mAP@.5: 0.0340
Epoch 112 - Train Loss: 0.246336, Val Loss: 0.242821, mAP@.5:.95: 0.0428, mAP@.5: 0.0434
Epoch 113 - Train Loss: 0.246869, Val Loss: 0.244911, mAP@.5:.95: 0.0450, mAP@.5: 0.0445
Epoch 114 - Train Loss: 0.245778, Val Loss: 0.241968, mAP@.5:.95: 0.0437, mAP@.5: 0.0415
Epoch 115 - Train Loss: 0.247702, Val Loss: 0.242372, mAP@.5:.95: 0.0459, mAP@.5: 0.0448
Epoch 116 - Train Loss: 0.246992, Val Loss: 0.245185, mAP@.5:.95: 0.0435, mAP@.5: 0.0425
Epoch 117 - Train Loss: 0.245938, Val Loss: 0.244237, mAP@.5:.95: 0.0441, mAP@.5: 0.0434
Epoch 118 - Train Loss: 0.246090, Val Loss: 0.242106, mAP@.5:.95: 0.0440, mAP@.5: 0.0449
Epoch 119 - Train Loss: 0.245499, Val Loss: 0.243376, mAP@.5:.95: 0.0468, mAP@.5: 0.0476
Epoch 120 - Train Loss: 0.246108, Val Loss: 0.243748, mAP@.5:.95: 0.0476, mAP@.5: 0.0482
Epoch 121 - Train Loss: 0.246014, Val Loss: 0.243707, mAP@.5:.95: 0.0423, mAP@.5: 0.0452
Epoch 122 - Train Loss: 0.246665, Val Loss: 0.242705, mAP@.5:.95: 0.0411, mAP@.5: 0.0416
Epoch 123 - Train Loss: 0.245891, Val Loss: 0.242573, mAP@.5:.95: 0.0443, mAP@.5: 0.0435
Epoch 124 - Train Loss: 0.246365, Val Loss: 0.243106, mAP@.5:.95: 0.0436, mAP@.5: 0.0433
Epoch 125 - Train Loss: 0.244320, Val Loss: 0.245244, mAP@.5:.95: 0.0422, mAP@.5: 0.0435
Epoch 126 - Train Loss: 0.246482, Val Loss: 0.241966, mAP@.5:.95: 0.0458, mAP@.5: 0.0490
Epoch 127 - Train Loss: 0.246233, Val Loss: 0.241801, mAP@.5:.95: 0.0421, mAP@.5: 0.0415
Epoch 128 - Train Loss: 0.246001, Val Loss: 0.241705, mAP@.5:.95: 0.0460, mAP@.5: 0.0461
Epoch 129 - Train Loss: 0.244512, Val Loss: 0.244222, mAP@.5:.95: 0.0439, mAP@.5: 0.0440
Epoch 130 - Train Loss: 0.247535, Val Loss: 0.241809, mAP@.5:.95: 0.0415, mAP@.5: 0.0414
Epoch 131 - Train Loss: 0.247122, Val Loss: 0.243691, mAP@.5:.95: 0.0431, mAP@.5: 0.0424
Epoch 132 - Train Loss: 0.245880, Val Loss: 0.240743, mAP@.5:.95: 0.0483, mAP@.5: 0.0435
Epoch 133 - Train Loss: 0.246716, Val Loss: 0.241123, mAP@.5:.95: 0.0443, mAP@.5: 0.0442
Epoch 134 - Train Loss: 0.247903, Val Loss: 0.243098, mAP@.5:.95: 0.0433, mAP@.5: 0.0400
Epoch 135 - Train Loss: 0.245504, Val Loss: 0.243263, mAP@.5:.95: 0.0444, mAP@.5: 0.0437
Epoch 136 - Train Loss: 0.245128, Val Loss: 0.242045, mAP@.5:.95: 0.0453, mAP@.5: 0.0437
Epoch 137 - Train Loss: 0.245958, Val Loss: 0.239989, mAP@.5:.95: 0.0464, mAP@.5: 0.0455
Epoch 138 - Train Loss: 0.245436, Val Loss: 0.241095, mAP@.5:.95: 0.0444, mAP@.5: 0.0422
Epoch 139 - Train Loss: 0.246233, Val Loss: 0.243197, mAP@.5:.95: 0.0469, mAP@.5: 0.0460
Epoch 140 - Train Loss: 0.247261, Val Loss: 0.243738, mAP@.5:.95: 0.0476, mAP@.5: 0.0471
Epoch 141 - Train Loss: 0.245226, Val Loss: 0.242989, mAP@.5:.95: 0.0475, mAP@.5: 0.0447
Epoch 142 - Train Loss: 0.246551, Val Loss: 0.243983, mAP@.5:.95: 0.0495, mAP@.5: 0.0493
Epoch 143 - Train Loss: 0.248288, Val Loss: 0.240743, mAP@.5:.95: 0.0467, mAP@.5: 0.0482
Epoch 144 - Train Loss: 0.245005, Val Loss: 0.241617, mAP@.5:.95: 0.0450, mAP@.5: 0.0449
Epoch 145 - Train Loss: 0.246872, Val Loss: 0.242412, mAP@.5:.95: 0.0478, mAP@.5: 0.0455
Epoch 146 - Train Loss: 0.244830, Val Loss: 0.242849, mAP@.5:.95: 0.0479, mAP@.5: 0.0446
Epoch 147 - Train Loss: 0.247024, Val Loss: 0.241200, mAP@.5:.95: 0.0467, mAP@.5: 0.0441
Epoch 148 - Train Loss: 0.245616, Val Loss: 0.242746, mAP@.5:.95: 0.0455, mAP@.5: 0.0455
Epoch 149 - Train Loss: 0.245593, Val Loss: 0.242915, mAP@.5:.95: 0.0477, mAP@.5: 0.0515
Epoch 150 - Train Loss: 0.245623, Val Loss: 0.242109, mAP@.5:.95: 0.0474, mAP@.5: 0.0473
Epoch 151 - Train Loss: 0.245964, Val Loss: 0.244208, mAP@.5:.95: 0.0470, mAP@.5: 0.0466
Epoch 152 - Train Loss: 0.245057, Val Loss: 0.242511, mAP@.5:.95: 0.0493, mAP@.5: 0.0456
Epoch 153 - Train Loss: 0.245915, Val Loss: 0.242213, mAP@.5:.95: 0.0482, mAP@.5: 0.0464
Epoch 154 - Train Loss: 0.246160, Val Loss: 0.240879, mAP@.5:.95: 0.0446, mAP@.5: 0.0427
Epoch 155 - Train Loss: 0.244071, Val Loss: 0.242171, mAP@.5:.95: 0.0478, mAP@.5: 0.0480
Epoch 156 - Train Loss: 0.245939, Val Loss: 0.243337, mAP@.5:.95: 0.0486, mAP@.5: 0.0495
Epoch 157 - Train Loss: 0.247101, Val Loss: 0.244572, mAP@.5:.95: 0.0474, mAP@.5: 0.0470
Epoch 158 - Train Loss: 0.245195, Val Loss: 0.242371, mAP@.5:.95: 0.0468, mAP@.5: 0.0470
Epoch 159 - Train Loss: 0.245616, Val Loss: 0.243828, mAP@.5:.95: 0.0476, mAP@.5: 0.0490
Epoch 160 - Train Loss: 0.246160, Val Loss: 0.242385, mAP@.5:.95: 0.0455, mAP@.5: 0.0422
Epoch 161 - Train Loss: 0.244986, Val Loss: 0.241381, mAP@.5:.95: 0.0481, mAP@.5: 0.0479
Epoch 162 - Train Loss: 0.247808, Val Loss: 0.240814, mAP@.5:.95: 0.0490, mAP@.5: 0.0480
Epoch 163 - Train Loss: 0.246368, Val Loss: 0.242874, mAP@.5:.95: 0.0507, mAP@.5: 0.0533
Epoch 164 - Train Loss: 0.244677, Val Loss: 0.242682, mAP@.5:.95: 0.0444, mAP@.5: 0.0413
Epoch 165 - Train Loss: 0.245248, Val Loss: 0.243111, mAP@.5:.95: 0.0510, mAP@.5: 0.0513
Epoch 166 - Train Loss: 0.245389, Val Loss: 0.241844, mAP@.5:.95: 0.0481, mAP@.5: 0.0484
Epoch 167 - Train Loss: 0.246230, Val Loss: 0.244144, mAP@.5:.95: 0.0495, mAP@.5: 0.0491
Epoch 168 - Train Loss: 0.246566, Val Loss: 0.240120, mAP@.5:.95: 0.0515, mAP@.5: 0.0529
Epoch 169 - Train Loss: 0.245516, Val Loss: 0.240416, mAP@.5:.95: 0.0475, mAP@.5: 0.0462
Epoch 170 - Train Loss: 0.245902, Val Loss: 0.241546, mAP@.5:.95: 0.0499, mAP@.5: 0.0509
Epoch 171 - Train Loss: 0.244694, Val Loss: 0.241223, mAP@.5:.95: 0.0461, mAP@.5: 0.0454
Epoch 172 - Train Loss: 0.244934, Val Loss: 0.242270, mAP@.5:.95: 0.0521, mAP@.5: 0.0559
Epoch 173 - Train Loss: 0.243799, Val Loss: 0.241956, mAP@.5:.95: 0.0511, mAP@.5: 0.0496
Epoch 174 - Train Loss: 0.245297, Val Loss: 0.241590, mAP@.5:.95: 0.0484, mAP@.5: 0.0475
Epoch 175 - Train Loss: 0.244494, Val Loss: 0.242737, mAP@.5:.95: 0.0495, mAP@.5: 0.0494
Epoch 176 - Train Loss: 0.243907, Val Loss: 0.241671, mAP@.5:.95: 0.0498, mAP@.5: 0.0497
Epoch 177 - Train Loss: 0.244856, Val Loss: 0.243221, mAP@.5:.95: 0.0504, mAP@.5: 0.0515
Epoch 178 - Train Loss: 0.245322, Val Loss: 0.243167, mAP@.5:.95: 0.0497, mAP@.5: 0.0493
Epoch 179 - Train Loss: 0.244704, Val Loss: 0.242543, mAP@.5:.95: 0.0526, mAP@.5: 0.0550
Epoch 180 - Train Loss: 0.244163, Val Loss: 0.242334, mAP@.5:.95: 0.0466, mAP@.5: 0.0453
Epoch 181 - Train Loss: 0.245721, Val Loss: 0.240351, mAP@.5:.95: 0.0533, mAP@.5: 0.0562
Epoch 182 - Train Loss: 0.243850, Val Loss: 0.242721, mAP@.5:.95: 0.0531, mAP@.5: 0.0569
Epoch 183 - Train Loss: 0.245973, Val Loss: 0.243873, mAP@.5:.95: 0.0489, mAP@.5: 0.0481
Epoch 184 - Train Loss: 0.246563, Val Loss: 0.242187, mAP@.5:.95: 0.0524, mAP@.5: 0.0576
Epoch 185 - Train Loss: 0.245563, Val Loss: 0.240725, mAP@.5:.95: 0.0554, mAP@.5: 0.0616
Epoch 186 - Train Loss: 0.244613, Val Loss: 0.238060, mAP@.5:.95: 0.0520, mAP@.5: 0.0524
Epoch 187 - Train Loss: 0.244748, Val Loss: 0.242535, mAP@.5:.95: 0.0518, mAP@.5: 0.0549
Epoch 188 - Train Loss: 0.244843, Val Loss: 0.241387, mAP@.5:.95: 0.0536, mAP@.5: 0.0585
Epoch 189 - Train Loss: 0.244110, Val Loss: 0.244268, mAP@.5:.95: 0.0478, mAP@.5: 0.0434
Epoch 190 - Train Loss: 0.243923, Val Loss: 0.241743, mAP@.5:.95: 0.0532, mAP@.5: 0.0576
Epoch 191 - Train Loss: 0.244991, Val Loss: 0.240520, mAP@.5:.95: 0.0532, mAP@.5: 0.0598
Epoch 192 - Train Loss: 0.245360, Val Loss: 0.240168, mAP@.5:.95: 0.0504, mAP@.5: 0.0480
Epoch 193 - Train Loss: 0.243538, Val Loss: 0.240911, mAP@.5:.95: 0.0505, mAP@.5: 0.0485
Epoch 194 - Train Loss: 0.243934, Val Loss: 0.241595, mAP@.5:.95: 0.0491, mAP@.5: 0.0486
Epoch 195 - Train Loss: 0.243576, Val Loss: 0.240275, mAP@.5:.95: 0.0537, mAP@.5: 0.0583
Epoch 196 - Train Loss: 0.245034, Val Loss: 0.242623, mAP@.5:.95: 0.0545, mAP@.5: 0.0577
Epoch 197 - Train Loss: 0.244358, Val Loss: 0.241329, mAP@.5:.95: 0.0538, mAP@.5: 0.0586
Epoch 198 - Train Loss: 0.244049, Val Loss: 0.241186, mAP@.5:.95: 0.0541, mAP@.5: 0.0547
Epoch 199 - Train Loss: 0.244247, Val Loss: 0.241441, mAP@.5:.95: 0.0498, mAP@.5: 0.0488
Dragoooo! Dragooooooo! Dragoooooooooooo!
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0') tensor([0.1330, 0.1088, 0.0962, 0.0911, 0.0906, 0.0839, 0.0707, 0.0674, 0.0664,
        0.0656, 0.0612, 0.0600, 0.0584, 0.0570, 0.0566, 0.0537, 0.0530, 0.0510,
        0.0505], device='cuda:0')
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
       device='cuda:0') tensor([0.1430, 0.1065, 0.1055, 0.1013, 0.0972, 0.0836, 0.0782, 0.0781, 0.0721,
        0.0703, 0.0701, 0.0687, 0.0597, 0.0563, 0.0555, 0.0539, 0.0537, 0.0526,
        0.0511], device='cuda:0')
IoU Matrix:
 tensor([[0.0000],
        [0.0000],
        [0.1039],
        [0.4233],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0811],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0763],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000],
        [0.0000]])

🔍 Image 0: 19 boxes predicted
Score: 0.1805, Box: [ 83.06566 116.02831  84.7635  124.22526], Width: 1.70, Height: 8.20
Score: 0.1498, Box: [ 83.7682  115.99117  85.46242 124.10528], Width: 1.69, Height: 8.11
Score: 0.1381, Box: [ 82.09326  115.911125  83.79544  124.36612 ], Width: 1.70, Height: 8.45
Score: 0.1253, Box: [ 17.427729 117.70523   18.754835 123.39871 ], Width: 1.33, Height: 5.69
Score: 0.1134, Box: [ 19.812996 117.03962   21.269955 123.4366  ], Width: 1.46, Height: 6.40
Score: 0.0889, Box: [ 81.52067  116.29291   83.216545 124.71463 ], Width: 1.70, Height: 8.42
Score: 0.0885, Box: [ 17.820568 117.0734    19.33723  123.86379 ], Width: 1.52, Height: 6.79
Score: 0.0866, Box: [ 87.93919  115.15646   89.269485 120.84923 ], Width: 1.33, Height: 5.69
Score: 0.0827, Box: [ 88.62487 115.10437  89.88926 120.44457], Width: 1.26, Height: 5.34
Score: 0.0821, Box: [124.897446 116.16798  126.80551  125.191635], Width: 1.91, Height: 9.02
Score: 0.0720, Box: [ 18.938189 116.89881   20.459259 123.35992 ], Width: 1.52, Height: 6.46
Score: 0.0709, Box: [ 82.5841  115.20617  84.2393  123.55456], Width: 1.66, Height: 8.35
Score: 0.0666, Box: [125.801384 116.615005 127.80086  125.52809 ], Width: 2.00, Height: 8.91
Score: 0.0581, Box: [ 22.379107 116.37474   24.072218 123.28154 ], Width: 1.69, Height: 6.91
Score: 0.0559, Box: [ 89.85062  115.0684    91.11142  120.306366], Width: 1.26, Height: 5.24
Score: 0.0556, Box: [126.86455  116.461655 128.86847  125.62279 ], Width: 2.00, Height: 9.16
Score: 0.0550, Box: [211.20544 116.94904 212.75064 123.86879], Width: 1.55, Height: 6.92
Score: 0.0531, Box: [ 22.943474 118.255104  24.663767 125.56476 ], Width: 1.72, Height: 7.31
Score: 0.0527, Box: [209.88834 116.12122 211.38794 123.68111], Width: 1.50, Height: 7.56

🔍 Image 1: 22 boxes predicted
Score: 0.1182, Box: [ 81.35368 116.15915  82.96993 124.11269], Width: 1.62, Height: 7.95
Score: 0.1143, Box: [125.21927  115.97077  127.18332  125.836266], Width: 1.96, Height: 9.87
Score: 0.0900, Box: [211.38208 116.86699 212.89008 123.96437], Width: 1.51, Height: 7.10
Score: 0.0891, Box: [212.09627 116.28827 213.571   123.22577], Width: 1.47, Height: 6.94
Score: 0.0875, Box: [ 85.80499 116.48413  87.39623 123.90343], Width: 1.59, Height: 7.42
Score: 0.0779, Box: [ 82.87893  116.639626  84.5826   124.293365], Width: 1.70, Height: 7.65
Score: 0.0771, Box: [ 21.415867 116.65149   23.067438 123.59738 ], Width: 1.65, Height: 6.95
Score: 0.0743, Box: [ 22.712328 117.3875    24.482344 124.499466], Width: 1.77, Height: 7.11
Score: 0.0696, Box: [213.7687  115.43166 214.96756 120.33934], Width: 1.20, Height: 4.91
Score: 0.0692, Box: [208.5657   116.25532  210.02647  124.099846], Width: 1.46, Height: 7.84
Score: 0.0688, Box: [ 18.260073 117.211334  19.784555 124.061386], Width: 1.52, Height: 6.85
Score: 0.0674, Box: [ 84.57078 116.08541  86.14594 123.45006], Width: 1.58, Height: 7.36
Score: 0.0657, Box: [215.06812 115.42333 216.22968 120.31819], Width: 1.16, Height: 4.89
Score: 0.0642, Box: [208.95732  116.982475 210.55054  124.1848  ], Width: 1.59, Height: 7.20
Score: 0.0626, Box: [203.84296 116.65019 205.67896 125.64724], Width: 1.84, Height: 9.00
Score: 0.0612, Box: [128.70186  115.65214  130.58165  125.315384], Width: 1.88, Height: 9.66
Score: 0.0601, Box: [125.916794 116.38791  127.97499  126.01305 ], Width: 2.06, Height: 9.63
Score: 0.0583, Box: [129.17776 116.52156 131.18216 125.99028], Width: 2.00, Height: 9.47
Score: 0.0575, Box: [ 16.010952 117.635086  17.401623 123.69279 ], Width: 1.39, Height: 6.06
Score: 0.0573, Box: [127.05807  116.4326   129.11815  125.963264], Width: 2.06, Height: 9.53
Score: 0.0560, Box: [ 23.229822 118.68576   24.979883 125.627106], Width: 1.75, Height: 6.94
Score: 0.0541, Box: [128.01927 116.48969 130.0587  125.96276], Width: 2.04, Height: 9.47

🔍 Image 2: 31 boxes predicted
Score: 0.5865, Box: [ 95.533714 114.47092   96.66448  118.71074 ], Width: 1.13, Height: 4.24
Score: 0.5728, Box: [ 96.077415 114.312126  97.24715  118.93534 ], Width: 1.17, Height: 4.62
Score: 0.3687, Box: [ 95.701416 114.9635    96.94173  120.05422 ], Width: 1.24, Height: 5.09
Score: 0.3392, Box: [ 97.000885 114.4044    98.104225 118.55103 ], Width: 1.10, Height: 4.15
Score: 0.2220, Box: [ 94.79195  114.463974  95.934814 119.002365], Width: 1.14, Height: 4.54
Score: 0.1162, Box: [ 97.38993 113.62144  98.43099 117.42058], Width: 1.04, Height: 3.80
Score: 0.1122, Box: [ 84.45335  116.40748   86.11596  123.958664], Width: 1.66, Height: 7.55
Score: 0.1005, Box: [ 23.282087 117.10229   24.989859 124.35985 ], Width: 1.71, Height: 7.26
Score: 0.0989, Box: [210.25102  116.452194 211.6783   123.9787  ], Width: 1.43, Height: 7.53
Score: 0.0962, Box: [209.40494  116.498344 210.83018  123.941284], Width: 1.43, Height: 7.44
Score: 0.0893, Box: [ 85.126434 116.64869   86.786766 124.15743 ], Width: 1.66, Height: 7.51
Score: 0.0863, Box: [ 86.87022  116.062096  88.47196  123.63288 ], Width: 1.60, Height: 7.57
Score: 0.0778, Box: [ 85.6639   116.658844  87.38859  124.20599 ], Width: 1.72, Height: 7.55
Score: 0.0716, Box: [ 94.13647  114.9953    95.38476  120.157875], Width: 1.25, Height: 5.16
Score: 0.0694, Box: [130.92798 116.67342 132.938   125.82057], Width: 2.01, Height: 9.15
Score: 0.0666, Box: [ 81.666176 116.0312    83.41992  123.54731 ], Width: 1.75, Height: 7.52
Score: 0.0659, Box: [ 86.58799  115.421616  88.0658   121.9101  ], Width: 1.48, Height: 6.49
Score: 0.0657, Box: [ 21.395935 117.11217   23.000092 124.19167 ], Width: 1.60, Height: 7.08
Score: 0.0633, Box: [129.64607  116.575584 131.64601  125.93272 ], Width: 2.00, Height: 9.36
Score: 0.0626, Box: [ 84.94915 115.53794  86.4568  122.20399], Width: 1.51, Height: 6.67
Score: 0.0596, Box: [ 83.69637 116.84595  85.44697 124.61473], Width: 1.75, Height: 7.77
Score: 0.0590, Box: [ 72.73168  118.30899   74.69529  125.796394], Width: 1.96, Height: 7.49
Score: 0.0569, Box: [213.22147  115.365654 214.45645  120.51893 ], Width: 1.23, Height: 5.15
Score: 0.0562, Box: [208.05109  116.52492  209.45708  123.971855], Width: 1.41, Height: 7.45
Score: 0.0561, Box: [ 93.296646 114.511444  94.43611  119.12658 ], Width: 1.14, Height: 4.62
Score: 0.0560, Box: [ 92.53561  114.608025  93.70972  119.24682 ], Width: 1.17, Height: 4.64
Score: 0.0560, Box: [ 90.38613  115.13903   91.687355 120.63126 ], Width: 1.30, Height: 5.49
Score: 0.0559, Box: [ 17.119406 116.82466   18.548386 123.71785 ], Width: 1.43, Height: 6.89
Score: 0.0547, Box: [128.84163  116.59355  130.84428  125.868004], Width: 2.00, Height: 9.27
Score: 0.0543, Box: [ 16.660385 116.936035  17.978838 123.02417 ], Width: 1.32, Height: 6.09
Score: 0.0510, Box: [ 80.88715 116.14803  82.5852  123.67184], Width: 1.70, Height: 7.52

🔍 Image 3: 24 boxes predicted
Score: 0.1863, Box: [ 80.121346 118.13134   81.94389  125.585556], Width: 1.82, Height: 7.45
Score: 0.1536, Box: [ 80.89829  117.78992   82.69519  125.245705], Width: 1.80, Height: 7.46
Score: 0.1417, Box: [ 81.45641 118.30931  83.26948 125.69988], Width: 1.81, Height: 7.39
Score: 0.0932, Box: [213.51141  116.267136 215.2113   124.31591 ], Width: 1.70, Height: 8.05
Score: 0.0918, Box: [211.70242 116.19626 213.38715 124.21753], Width: 1.68, Height: 8.02
Score: 0.0863, Box: [215.5637   115.17367  216.94186  121.199326], Width: 1.38, Height: 6.03
Score: 0.0848, Box: [215.05476 116.00961 216.54141 123.82251], Width: 1.49, Height: 7.81
Score: 0.0817, Box: [208.57874 116.98066 210.3832  125.85892], Width: 1.80, Height: 8.88
Score: 0.0756, Box: [ 18.41279  117.836525  19.922003 124.13769 ], Width: 1.51, Height: 6.30
Score: 0.0722, Box: [216.40935 115.26386 217.79208 121.93972], Width: 1.38, Height: 6.68
Score: 0.0719, Box: [212.72101 116.6865  214.46968 124.88345], Width: 1.75, Height: 8.20
Score: 0.0715, Box: [ 19.708359 116.75651   21.091269 123.390594], Width: 1.38, Height: 6.63
Score: 0.0702, Box: [209.84814  117.205574 211.60867  125.371346], Width: 1.76, Height: 8.17
Score: 0.0676, Box: [210.52612  117.303345 212.34776  125.406204], Width: 1.82, Height: 8.10
Score: 0.0641, Box: [213.94904  115.077065 215.21321  121.00385 ], Width: 1.26, Height: 5.93
Score: 0.0635, Box: [214.53152  115.428024 215.91043  122.26186 ], Width: 1.38, Height: 6.83
Score: 0.0617, Box: [207.58788  117.02363  209.43416  125.813156], Width: 1.85, Height: 8.79
Score: 0.0601, Box: [215.00209 115.08633 216.27258 120.95673], Width: 1.27, Height: 5.87
Score: 0.0550, Box: [ 79.10501  118.72069   80.90298  125.661026], Width: 1.80, Height: 6.94
Score: 0.0536, Box: [ 20.812246 116.88165   22.276966 123.50392 ], Width: 1.46, Height: 6.62
Score: 0.0536, Box: [ 29.749706 119.45705   31.565203 125.80322 ], Width: 1.82, Height: 6.35
Score: 0.0521, Box: [ 31.961592 119.67706   33.84462  125.86933 ], Width: 1.88, Height: 6.19
Score: 0.0513, Box: [212.0879  117.21062 213.86606 125.34704], Width: 1.78, Height: 8.14
Score: 0.0506, Box: [217.12672 115.25232 218.50047 122.05898], Width: 1.37, Height: 6.81

🔍 Image 4: 24 boxes predicted
Score: 0.4630, Box: [ 92.12435  115.02345   93.50622  120.760185], Width: 1.38, Height: 5.74
Score: 0.4487, Box: [ 91.06279  115.03744   92.398445 120.67649 ], Width: 1.34, Height: 5.64
Score: 0.2498, Box: [ 92.52142 114.28934  93.7113  119.25223], Width: 1.19, Height: 4.96
Score: 0.2301, Box: [ 91.63251 115.00223  93.15553 122.25246], Width: 1.52, Height: 7.25
Score: 0.1220, Box: [213.96455  116.251976 215.43246  123.41476 ], Width: 1.47, Height: 7.16
Score: 0.1177, Box: [ 92.98241 114.27358  94.19215 119.07234], Width: 1.21, Height: 4.80
Score: 0.1024, Box: [ 90.43052  115.082275  91.99104  122.405136], Width: 1.56, Height: 7.32
Score: 0.0878, Box: [ 20.260098 117.07718   21.574848 123.08944 ], Width: 1.31, Height: 6.01
Score: 0.0836, Box: [ 87.27937  116.50236   88.959984 124.1573  ], Width: 1.68, Height: 7.65
Score: 0.0786, Box: [ 19.1901   116.857506  20.683855 123.35098 ], Width: 1.49, Height: 6.49
Score: 0.0699, Box: [ 82.91204 116.50232  84.66683 124.52101], Width: 1.75, Height: 8.02
Score: 0.0686, Box: [ 88.092926 116.016335  89.69571  123.61335 ], Width: 1.60, Height: 7.60
Score: 0.0656, Box: [207.40073 116.23716 209.0136  123.77531], Width: 1.61, Height: 7.54
Score: 0.0642, Box: [129.3372  116.42756 131.28703 126.03831], Width: 1.95, Height: 9.61
Score: 0.0641, Box: [ 81.62046 117.24851  83.59647 125.60181], Width: 1.98, Height: 8.35
Score: 0.0602, Box: [130.62617 116.46015 132.56924 126.0124 ], Width: 1.94, Height: 9.55
Score: 0.0599, Box: [212.44751 116.92624 213.9695  124.20903], Width: 1.52, Height: 7.28
Score: 0.0591, Box: [ 92.870926 115.13156   94.37699  122.25017 ], Width: 1.51, Height: 7.12
Score: 0.0571, Box: [ 80.66169 117.4071   82.70912 125.61868], Width: 2.05, Height: 8.21
Score: 0.0544, Box: [210.52164 116.59477 212.08618 124.17159], Width: 1.56, Height: 7.58
Score: 0.0536, Box: [214.85791  115.23043  216.16733  121.301414], Width: 1.31, Height: 6.07
Score: 0.0515, Box: [ 17.510908 116.26001   18.893028 122.88585 ], Width: 1.38, Height: 6.63
Score: 0.0515, Box: [ 22.758095 116.61934   24.408089 123.6141  ], Width: 1.65, Height: 6.99
Score: 0.0508, Box: [206.66304 117.38158 208.53278 125.7213 ], Width: 1.87, Height: 8.34

🔍 Image 5: 17 boxes predicted
Score: 0.0974, Box: [ 87.26914  116.062874  88.90066  123.8159  ], Width: 1.63, Height: 7.75
Score: 0.0834, Box: [125.61493  115.87075  127.67511  125.728905], Width: 2.06, Height: 9.86
Score: 0.0785, Box: [ 88.156136 116.4697    89.79695  124.1606  ], Width: 1.64, Height: 7.69
Score: 0.0756, Box: [214.17657  116.54844  215.57828  123.320045], Width: 1.40, Height: 6.77
Score: 0.0752, Box: [ 90.33137 115.02026  91.66089 120.70392], Width: 1.33, Height: 5.68
Score: 0.0737, Box: [ 91.176735 114.97507   92.50918  120.52896 ], Width: 1.33, Height: 5.55
Score: 0.0724, Box: [203.63333  119.05441  205.44783  125.690796], Width: 1.81, Height: 6.64
Score: 0.0659, Box: [207.40329 116.78108 208.86029 124.11052], Width: 1.46, Height: 7.33
Score: 0.0639, Box: [ 91.181984 115.87564   92.71667  123.30891 ], Width: 1.53, Height: 7.43
Score: 0.0620, Box: [ 82.555954 115.76826   84.30831  123.71104 ], Width: 1.75, Height: 7.94
Score: 0.0615, Box: [ 81.85789  117.10524   83.75261  125.227295], Width: 1.89, Height: 8.12
Score: 0.0593, Box: [ 22.992634 116.55195   24.697678 124.05118 ], Width: 1.71, Height: 7.50
Score: 0.0562, Box: [ 80.38986  117.16307   82.391716 125.52007 ], Width: 2.00, Height: 8.36
Score: 0.0559, Box: [ 22.723495 118.51908   24.514008 125.712746], Width: 1.79, Height: 7.19
Score: 0.0546, Box: [ 99.31178  113.50014  100.327705 117.39949 ], Width: 1.02, Height: 3.90
Score: 0.0545, Box: [ 84.46821  116.371376  86.25697  124.40634 ], Width: 1.79, Height: 8.03
Score: 0.0530, Box: [ 72.30942 119.03879  74.21475 125.86337], Width: 1.91, Height: 6.82

🔍 Image 6: 18 boxes predicted
Score: 0.3879, Box: [ 88.44114 115.64156  90.05296 123.53509], Width: 1.61, Height: 7.89
Score: 0.3208, Box: [ 89.59249  115.671745  91.18783  123.41908 ], Width: 1.60, Height: 7.75
Score: 0.2748, Box: [ 88.96566  114.86584   90.52398  122.544205], Width: 1.56, Height: 7.68
Score: 0.1436, Box: [ 87.777214 114.96777   89.337326 122.51555 ], Width: 1.56, Height: 7.55
Score: 0.1148, Box: [ 90.101715 114.91302   91.66115  122.470024], Width: 1.56, Height: 7.56
Score: 0.1091, Box: [123.22365  114.408104 124.69142  121.35456 ], Width: 1.47, Height: 6.95
Score: 0.0741, Box: [ 72.03027 118.14797  73.91901 125.89013], Width: 1.89, Height: 7.74
Score: 0.0707, Box: [ 72.76671  117.197586  74.69655  125.71766 ], Width: 1.93, Height: 8.52
Score: 0.0675, Box: [ 87.40544 116.62961  89.09889 124.6942 ], Width: 1.69, Height: 8.06
Score: 0.0667, Box: [123.62986 115.22412 125.37079 124.01751], Width: 1.74, Height: 8.79
Score: 0.0643, Box: [122.082886 114.4151   123.55925  121.22077 ], Width: 1.48, Height: 6.81
Score: 0.0600, Box: [ 21.313389 116.62379   22.805239 123.6742  ], Width: 1.49, Height: 7.05
Score: 0.0599, Box: [ 79.703674 116.96537   81.677925 125.501114], Width: 1.97, Height: 8.54
Score: 0.0584, Box: [ 78.6579   117.08484   80.60489  125.479744], Width: 1.95, Height: 8.39
Score: 0.0560, Box: [ 89.043396 116.38049   90.887726 125.44483 ], Width: 1.84, Height: 9.06
Score: 0.0530, Box: [210.48799 116.40597 211.96684 123.29829], Width: 1.48, Height: 6.89
Score: 0.0527, Box: [ 80.43671  117.04309   82.378716 125.432   ], Width: 1.94, Height: 8.39
Score: 0.0510, Box: [ 81.576996 116.74846   83.417435 125.28565 ], Width: 1.84, Height: 8.54

🔍 Image 7: 22 boxes predicted
Score: 0.4862, Box: [127.84869  115.243835 129.84006  125.26145 ], Width: 1.99, Height: 10.02
Score: 0.4166, Box: [126.78335 115.86261 128.86964 125.96954], Width: 2.09, Height: 10.11
Score: 0.1248, Box: [128.75862 116.22964 130.81079 126.02099], Width: 2.05, Height: 9.79
Score: 0.1152, Box: [ 84.18451 116.81728  86.12604 125.41252], Width: 1.94, Height: 8.60
Score: 0.1027, Box: [ 83.23518  116.72218   85.120575 125.40111 ], Width: 1.89, Height: 8.68
Score: 0.0908, Box: [ 18.40356  117.0069    19.854506 123.610466], Width: 1.45, Height: 6.60
Score: 0.0901, Box: [ 91.14492 114.9243   92.53441 120.9676 ], Width: 1.39, Height: 6.04
Score: 0.0793, Box: [ 82.62042 116.81962  84.49569 125.57105], Width: 1.88, Height: 8.75
Score: 0.0721, Box: [ 18.8138  117.50052  20.33584 124.02652], Width: 1.52, Height: 6.53
Score: 0.0684, Box: [ 84.63953  115.27163   86.242386 122.94027 ], Width: 1.60, Height: 7.67
Score: 0.0663, Box: [ 17.088717 116.87189   18.534533 123.4087  ], Width: 1.45, Height: 6.54
Score: 0.0613, Box: [124.53407 115.85362 126.59185 125.75652], Width: 2.06, Height: 9.90
Score: 0.0599, Box: [ 90.76353  115.60895   92.408066 123.627655], Width: 1.64, Height: 8.02
Score: 0.0595, Box: [ 85.03107  115.616905  86.820335 123.85927 ], Width: 1.79, Height: 8.24
Score: 0.0559, Box: [ 81.786476 116.860176  83.86313  125.54491 ], Width: 2.08, Height: 8.68
Score: 0.0549, Box: [125.460884 115.95891  127.411995 125.309494], Width: 1.95, Height: 9.35
Score: 0.0540, Box: [ 85.48246 116.64643  87.33927 124.95681], Width: 1.86, Height: 8.31
Score: 0.0533, Box: [ 96.85985  114.26155   98.044106 119.041   ], Width: 1.18, Height: 4.78
Score: 0.0529, Box: [ 81.10677 116.72819  83.11162 125.64852], Width: 2.00, Height: 8.92
Score: 0.0527, Box: [ 97.30301  114.221306  98.476295 118.68526 ], Width: 1.17, Height: 4.46
Score: 0.0506, Box: [214.50273  115.50468  215.72127  120.380455], Width: 1.22, Height: 4.88
Score: 0.0501, Box: [ 15.177721 117.38546   16.634512 123.59957 ], Width: 1.46, Height: 6.21
mAP at IoU=0.50:0.95:  tensor(0.0498)
mAP at IoU=0.50:  tensor(0.0488)
No true positives detected — precision/recall not available.
Recall not available — likely no matched predictions.
Inference complete and results saved!
